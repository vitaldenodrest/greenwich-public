{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314ddce9",
   "metadata": {},
   "source": [
    "Copyright (c) 2026 [Vital de Nodrest]\n",
    "\n",
    "The code (Python, shell and PowerShell cells) is under MIT license. Feel free to share and experiment! See LICENSE-CODE.txt in the project root for more information.\n",
    "\n",
    "Due to their time-consuming and didactic nature, text & image contents are under CC BY-NC-SA 4.0 license. Removal of the author's name or redistribution without credit is prohibited. See LICENSE-DOCS.txt in the project root for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d874a5",
   "metadata": {},
   "source": [
    "# Introduction to PINNs for Helmholtz problems\n",
    "\n",
    "This notebook aims at being a basic introduction to PINNs using the example of Helmholtz wave propagation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83f38a",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "You can use any recent Python configuration to run this notebook.\n",
    "\n",
    "The packages are specified in the [requirements](../requirements.txt) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b25252",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6369e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e05e1",
   "metadata": {},
   "source": [
    "The next subsections provide different configuration scripts depending on your device.\n",
    "\n",
    "You can uncomment and run the ones you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd15809",
   "metadata": {},
   "source": [
    "The models in this notebook are:\n",
    "- small (<< 100k parameters)\n",
    "- linear (no attention, convolution, *etc*)\n",
    "- trained on small batch sizes (< 32 most of the time)\n",
    "\n",
    "*Ergo*, computations should run way faster on CPU than on other hardware. The configuration will be way simpler, as PyTorch runs on CPU by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0963d9",
   "metadata": {},
   "source": [
    "### Linux, CPU\n",
    "\n",
    "Configuration for Linux devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf662cd1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea9726",
   "metadata": {},
   "source": [
    "### Mac, CPU\n",
    "\n",
    "Configuration for Mac devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40146ca",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db154d7d",
   "metadata": {},
   "source": [
    "### Windows, CPU\n",
    "\n",
    "Configuration for Windows devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39b740",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19f745",
   "metadata": {},
   "source": [
    "### Something went wrong?\n",
    "\n",
    "If your situation doesn't fit any of the subsections, see the [PyTorch installation tutorial](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c5352",
   "metadata": {},
   "source": [
    "## About PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1baef",
   "metadata": {},
   "source": [
    "### Universal approximation theorem\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae7c5f",
   "metadata": {},
   "source": [
    "### Automatic differentiation and physical losses\n",
    "\n",
    "TODO automatic differentiation & backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd0bc8",
   "metadata": {},
   "source": [
    "PINNs are based on the following idea: what if we ran the backpropagation a step further, thus reaching the input layer?\n",
    "\n",
    "In the context of a neural network with physical coordinates as inputs and field variables as outputs, this allows us to compute physically relevant partial derivatives of the model.\n",
    "\n",
    "They are automatically differentiable themselves, allowing us to run the backpropagation again, yielding higher order derivatives.\n",
    "\n",
    "These partial derivatives can then be used to compute the residuals of a PDE problem. These residuals themselves are automatically differentiable, enabling adjustment of the model weights *via* backpropagation again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e204e16",
   "metadata": {},
   "source": [
    "## A 1D Helmholtz problem\n",
    "\n",
    "A 1D example problem means fast computations on CPU and highly visual results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e68b5",
   "metadata": {},
   "source": [
    "Let's consider the problem:\n",
    "$$\n",
    "\\frac{d^2u}{dx^2} + k^2 u = 0 ~~ \\text{in} ~ ]0,1[\n",
    "$$\n",
    "\n",
    "Where $k$ is a real-valued wavenumber. Conveniently, let's suppose that it is not $0$ or a multiple of $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee27cb",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "Thanks to usual results about second-order linear differential equations with constant coefficients, we know that solutions to this problem take the following form, with $c_1$ and $c_2$ two constant coefficients:\n",
    "$$\n",
    "u(x) = c_1 \\cos{kx} + c_2 \\sin{kx}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f36e1",
   "metadata": {},
   "source": [
    "Let's use $\\frac{du}{dx}(0) = 0$ and $\\frac{du}{dx}(1) = - k \\sin{k}$ as boundary conditions. They don't necessarily represent an interesting physical phenomenon, but at least they ensure $c_1 = 1$ and $c_2 = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0700769",
   "metadata": {},
   "source": [
    "Consequently, our problem is well-posed and its only solution is $u \\in [0,1] \\rightarrow \\mathbb{R}$ such that:\n",
    "$$\n",
    "\\forall x \\in [0,1], ~ u(x) = \\cos{kx}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a343ba7",
   "metadata": {},
   "source": [
    "This exact solution would not be available in general. It will be used for test purposes, but it will be ignored during training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a22020",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "The following chapter goes over the PyTorch implementation of a PINN solver using the Helmholtz example.\n",
    "\n",
    "Feel free to play with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f86029",
   "metadata": {},
   "source": [
    "### Helmholtz PDE problem\n",
    "\n",
    "To implement a PDE problem to be solved using a PINN, one needs to write the expressions of its residuals. The model will be trained to minimize these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66679d86",
   "metadata": {},
   "source": [
    "These residuals based on partial derivatives are computed using automatic differentation (PyTorch autograd implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f115ac",
   "metadata": {},
   "source": [
    "Setting the problem wavenumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23a0b",
   "metadata": {},
   "source": [
    "Notation reminder: $f_{\\theta}$ is the model with parameters $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a723759",
   "metadata": {},
   "source": [
    "Residual for a point $x_i$ inside the domain $]0, 1[$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f_{\\theta}}{\\partial x^2}(x_i) + k^2 f_{\\theta}(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_pde(X: torch.Tensor, U: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Batch computing the model PDE residual in the domain ]0,1[.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): n model inputs.\n",
    "        U (torch.Tensor): n model outputs after respective inferences on X.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: n model PDE residuals.\n",
    "    \"\"\"\n",
    "\n",
    "    du_dx = torch.autograd.grad(outputs=U,\n",
    "                                inputs=X,\n",
    "                                grad_outputs=torch.ones_like(U), # Shape information for batches\n",
    "                                create_graph=True, # creating a graph for higher order derivatives\n",
    "                                retain_graph=True,\n",
    "                                )[0]\n",
    "    du_dxx = torch.autograd.grad(outputs=du_dx,\n",
    "                                 inputs=X,\n",
    "                                 grad_outputs=torch.ones_like(du_dx), # Shape information for batches\n",
    "                                 create_graph=True,\n",
    "    )[0]\n",
    "    return du_dxx + k**2 * U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36fff1",
   "metadata": {},
   "source": [
    "Implementing the residual for a point $x_i$ on the left boundary ($i.e.$ $0$ in 1D):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{\\theta}}{\\partial x}(0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bba9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_0(X, U):\n",
    "    du_dx = torch.autograd.grad(outputs=U,\n",
    "                                inputs=X,\n",
    "                                grad_outputs=torch.ones_like(U), # Shape information for batches\n",
    "                                create_graph=True,\n",
    "                                retain_graph=True,\n",
    "                                )[0]\n",
    "    return du_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71dc6b",
   "metadata": {},
   "source": [
    "Implementing the residual for a point $x_i$ on the right boundary ($i.e.$ $1$ in 1D):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_{\\theta}}{\\partial x}(1) + k \\sin{k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a833eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_1(X, U):\n",
    "    du_dx = torch.autograd.grad(outputs=U,\n",
    "                                inputs=X,\n",
    "                                grad_outputs=torch.ones_like(U), # Shape information for batches\n",
    "                                create_graph=True,\n",
    "                                retain_graph=True,\n",
    "                                )[0]\n",
    "    return du_dx + k * torch.sin(k * torch.ones_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def u(x):\n",
    "    return torch.cos(k*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255434",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "X_train_pde = torch.linspace(0, 1, 10 + 2, dtype=torch.float32)[1: -1].unsqueeze_(1)\n",
    "X_train_0 = torch.tensor([0], dtype=torch.float32).unsqueeze_(1) # left boundary\n",
    "X_train_1 = torch.tensor([1], dtype=torch.float32).unsqueeze_(1) # right boundary\n",
    "### Validation\n",
    "X_validation_pde = torch.linspace(0.01, 0.99, 100, dtype=torch.float32).unsqueeze_(1)\n",
    "X_validation_0 = torch.tensor([0], dtype=torch.float32).unsqueeze_(1)\n",
    "X_validation_1 = torch.tensor([1], dtype=torch.float32).unsqueeze_(1)\n",
    "### Test\n",
    "X_test = torch.linspace(0, 1, 1000, dtype=torch.float32).unsqueeze_(1)\n",
    "U_test_exact = u(X_test)\n",
    "U_test_exact_norm = torch.linalg.vector_norm(U_test_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34659bce",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b019e4",
   "metadata": {},
   "source": [
    "Let's initialize our neural network.\n",
    "\n",
    "A typical PINN takes the physical coordinates of the problem as an input and outputs the solution.\n",
    "In this example, the input is **x** (**1D** space variable) and the output is **u** (**1D** scalar output).\n",
    "\n",
    "There are many architectural possibilities for the neural network. The simplest choice is a uniform fully-connected neural network with tanh activation functions. By default, each connexion has a weight and a bias.\n",
    "\n",
    "We cannot use the tanh activation function after the last layer as we need a solution that can reach $1$ and $-1$.\n",
    "\n",
    "TODO plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb959f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNN(nn.Module):\n",
    "    \"\"\"Simplest fully connected neural network for our problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int=1, output_dim: int=1, width: int=20, hidden_layers: int=3):\n",
    "        \"\"\"Initalize the simplest fully-connected neural network.\n",
    "        All hidden layers have the same width.\n",
    "        All edges have a weight and a bias.\n",
    "        Tanh activation functions (except for the last layer).\n",
    "\n",
    "        Args:\n",
    "            input_dim (int, optional): Input dimension. Defaults to 1.\n",
    "            output_dim (int, optional): Output dimension. Defaults to 1.\n",
    "            width (int, optional): Width of the single layers. Defaults to 20.\n",
    "            hidden_layers (int, optional): Number of hidden layers. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        layers = [nn.Linear(input_dim, width), nn.Tanh()] # input -> hidden layer 1\n",
    "        # hidden layer i -> hidden layer i+1\n",
    "        for _ in range(hidden_layers-1):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(width, output_dim)) # last hidden layer -> output\n",
    "        self.stack = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059c511",
   "metadata": {},
   "source": [
    "Feel free to change the parameters, or even write your own class. As an example, how would you write a fully-connected neural network with hidden layers if varying sizes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe2a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNN(width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fc924",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a0c6d",
   "metadata": {},
   "source": [
    "Let's optimize our model parameters $\\theta$ (weight & biases) iteratively using the Adam algorithm with a learning rate of $.001$ to minimize the mean-square-error loss function:\n",
    "\n",
    "$$L_{\\text{MSE}} \\left(\\theta, (x_i)_{i=1}^N, (y_i)_{i=1}^N \\right) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( f_{\\theta}(x_i) - y_i \\right)^2 $$\n",
    "\n",
    "Where $f_{\\theta}$ is the model parametrized by the vector $\\theta$, and we consider $N$ training samples:\n",
    "- Training points $(x_i)_{i=1}^N$\n",
    "- Respective solutions $(y_i)_{i=1}^N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e65e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss\n",
    "loss = torch.nn.MSELoss() # the default reduction is \"mean\", dividing the loss by the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bc132",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP\n",
    "\n",
    "num_epochs = 5000 # !!!\n",
    "train_losses = torch.zeros((3, num_epochs))\n",
    "validation_losses = torch.zeros((4, num_epochs))\n",
    "test_metrics = torch.zeros((num_epochs))\n",
    "\n",
    "best_model_state = None\n",
    "best_validation_loss = torch.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    ## Training phase\n",
    "    \n",
    "    ### Configuration\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # reset previously accumulated gradients\n",
    "    \n",
    "    ### Optional reampling\n",
    "    #X_train_pde = torch.rand(size=(10,1)).unsqueeze_(1)\n",
    "    \n",
    "    ### Enable automatic differentiation for training and residuals audtodiff\n",
    "    X_train_pde.requires_grad_(True)\n",
    "    X_train_0.requires_grad_(True)\n",
    "    X_train_1.requires_grad_(True)\n",
    "    \n",
    "    ### Evaluate the model\n",
    "    U_train_pde = model(X_train_pde)\n",
    "    U_train_0 = model(X_train_0)\n",
    "    U_train_1 = model(X_train_1)\n",
    "    \n",
    "    ### Compute residuals\n",
    "    RES_train_pde = residual_pde(X_train_pde, U_train_pde)\n",
    "    RES_train_0 = residual_0(X_train_0, U_train_0)\n",
    "    RES_train_1 = residual_1(X_train_1, U_train_1)\n",
    "    \n",
    "    ### Evaluate losses and accumulate the parameter derivatives in place\n",
    "    train_loss_pde: torch.Tensor = loss(RES_train_pde, torch.zeros_like(RES_train_pde))\n",
    "    train_loss_pde.backward()\n",
    "    train_losses[0, epoch] = train_loss_pde.tolist()\n",
    "    \n",
    "    train_loss_0 = loss(RES_train_0, torch.zeros_like(RES_train_0))\n",
    "    train_loss_0.backward()\n",
    "    train_losses[1, epoch] = train_loss_0.tolist()\n",
    "    \n",
    "    train_loss_1 = loss(RES_train_1, torch.zeros_like(RES_train_1))\n",
    "    train_loss_1.backward()\n",
    "    train_losses[2, epoch] = train_loss_1.tolist()\n",
    "    \n",
    "    ### Perform optimizer step\n",
    "    optimizer.step()\n",
    "        \n",
    "        \n",
    "    ## Validation phase\n",
    "    \n",
    "    ### Configuration\n",
    "    model.eval()\n",
    "    \n",
    "    \"\"\"\n",
    "    torch.no_grad() cannot be used because computing \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Enable automatic differentiation for residuals audtodiff\n",
    "    X_validation_pde.requires_grad_(True)\n",
    "    X_validation_0.requires_grad_(True)\n",
    "    X_validation_1.requires_grad_(True)\n",
    "        \n",
    "    ### Evaluate the model\n",
    "    U_validation_pde = model(X_validation_pde)\n",
    "    U_validation_0 = model(X_validation_0)\n",
    "    U_validation_1 = model(X_validation_1)\n",
    "    \n",
    "    ### Compute residuals\n",
    "    RES_validation_pde = residual_pde(X_validation_pde, U_validation_pde)\n",
    "    RES_validation_0 = residual_0(X_validation_0, U_validation_0)\n",
    "    RES_validation_1 = residual_1(X_validation_1, U_validation_1)\n",
    "    \n",
    "    ### Evaluate loss\n",
    "    validation_loss_total = 0\n",
    "    \n",
    "    validation_loss_pde: torch.Tensor = loss(RES_validation_pde, torch.zeros_like(RES_validation_pde))\n",
    "    validation_losses[0, epoch] = validation_loss_pde.tolist()\n",
    "    validation_loss_total += validation_loss_pde.tolist()\n",
    "    \n",
    "    validation_loss_0: torch.Tensor = loss(RES_validation_0, torch.zeros_like(RES_validation_0))\n",
    "    validation_losses[1, epoch] = validation_loss_0.tolist()\n",
    "    validation_loss_total += validation_loss_0.tolist()\n",
    "    \n",
    "    validation_loss_1: torch.Tensor = loss(RES_validation_1, torch.zeros_like(RES_validation_1))\n",
    "    validation_losses[2, epoch] = validation_loss_1.tolist()\n",
    "    validation_loss_total += validation_loss_1.tolist()\n",
    "    \n",
    "    \n",
    "    validation_losses[3, epoch] = validation_loss_total\n",
    "        \n",
    "    ### Save the best model\n",
    "    if validation_loss_total < best_validation_loss:\n",
    "        best_validation_loss = validation_loss_total\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        \n",
    "\n",
    "    ## Test phase (optional in the loop)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        U_test = model(X_test)\n",
    "        test_metrics[epoch] = (torch.linalg.vector_norm(U_test - U_test_exact) / U_test_exact_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45164db9",
   "metadata": {},
   "source": [
    "### Displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dda593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d9984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.plot(train_losses[0, :], label=']0,1[ residual')\n",
    "ax1.plot(train_losses[1, :], label='0 residual')\n",
    "ax1.plot(train_losses[2, :], label ='1 residual')\n",
    "ax1.plot(torch.sum(train_losses, dim=0), label='total residual')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_title('Training losses')\n",
    "ax1.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e910b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.plot(validation_losses[0, :], label=']0,1[ residual')\n",
    "ax2.plot(validation_losses[1, :], label='0 residual')\n",
    "ax2.plot(validation_losses[2, :], label='1 residual')\n",
    "ax2.plot(validation_losses[3, :], label='total residual')\n",
    "ax2.legend()\n",
    "ax2.set_title('Validation losses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ed1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.plot(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eead3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Last model\n",
    "fig4, ax4 = plt.subplots()\n",
    "ax4.plot(X_test, U_test_exact)\n",
    "ax4.plot(X_test, U_test)\n",
    "\n",
    "u_min = torch.min(torch.min(U_test_exact), torch.min(U_test))\n",
    "u_max = torch.max(torch.max(U_test_exact), torch.max(U_test))\n",
    "for x in X_train_pde.detach().tolist() + X_train_0.detach().tolist() + X_train_1.detach().tolist():\n",
    "    ax4.plot([x]*2, [u_min, u_max], color='grey', linewidth=.3)\n",
    "\n",
    "last_str = f\"Last model test metric: {test_metrics[-1].tolist()}\"\n",
    "print(last_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b61772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Best model\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(best_model_state)\n",
    "    U_test = model(X_test)\n",
    "    best_test_metric = (torch.linalg.vector_norm(U_test - U_test_exact) / U_test_exact_norm)\n",
    "fig5, ax5 = plt.subplots()\n",
    "ax5.plot(X_test, U_test_exact)\n",
    "ax5.plot(X_test, U_test)\n",
    "best_str = f\"Best model test metric: {best_test_metric.tolist()}\"\n",
    "print(best_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dbf00a",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5dcd5",
   "metadata": {},
   "source": [
    "## Further research...\n",
    "\n",
    "Stochastic PDEs\n",
    "\n",
    "Multi-physics\n",
    "\n",
    "Implementations\n",
    "\n",
    "Gradient pathologies\n",
    "\n",
    "Loss functions with multiple components\n",
    "\n",
    "Hyperparameter optimization/choice\n",
    "\n",
    "Other neural network architectures\n",
    "\n",
    "Operator learning\n",
    "\n",
    "Inverse problems\n",
    "\n",
    "Parametrized PDEs\n",
    "\n",
    "Geometry-dependant resolution\n",
    "\n",
    "Hybrid approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131f2aa",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"ref1\"></a>\n",
    "[1] Maziar Raissi, Paris Perdikaris, George Em Karniadakis (2017). \"Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations\". [arXiv:1711.10561](https://arxiv.org/abs/1711.10561)\n",
    "\n",
    "<a id=\"ref2\"></a>\n",
    "[2] Sifan Wang, Yujun Teng, Paris Perdikaris (2020). \"Understanding and mitigating gradient pathologies in physics-informed neural networks\". [arXiv:2001.04536](https://arxiv.org/abs/2001.04536)\n",
    "\n",
    "<a id=\"ref3\"></a>\n",
    "[3] Lu, Lu, Pengzhan Jin, et George Em Karniadakis (2021). \"DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\". [Nature Machine Intelligence 3, nᵒ 3 (2021): 218‑29](https://doi.org/10.1038/s42256-021-00302-5)\n",
    "\n",
    "<a id=\"ref4\"></a>\n",
    "[4] Cho, Woojin, Minju Jo, Haksoo Lim, *et al* (2024). \"Parameterized Physics-informed Neural Networks for Parameterized PDEs\". [arXiv:2408.09446](https://doi.org/10.48550/arXiv.2408.09446)\n",
    "\n",
    "<a id=\"ref5\"></a>\n",
    "[5] Toscano, Juan Diego, Vivek Oommen, Alan John Varghese, *et al* (2025). \"From PINNs to PIKANs: Recent Advances in Physics-Informed Machine Learning\". [Machine Learning for Computational Science and Engineering 1, nᵒ 1 (2025): 15](https://doi.org/10.1007/s44379-025-00015-1)\n",
    "\n",
    "<a id=\"ref6\"></a>\n",
    "[6] Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em (2021). \"DeepXDE: A deep learning library for solving differential equations\". [SIAM Review 63, nᵒ 1 (2021): 208-228](https://doi.org/10.1137/19M1274067)\n",
    "\n",
    "<a id=\"ref7\"></a>\n",
    "[7] Jonathon Hare (accessed in 2026). \"Differentiate Automatically: An Introduction to Automatic Differentiation\". [University of Southampton handouts, COMP6248](https://comp6248.ecs.soton.ac.uk/handouts/autograd-handouts.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greenwich-public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
